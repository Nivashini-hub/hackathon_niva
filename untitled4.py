# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JyAB2o2jiKoOxf2hqA3_6g5dHQL1-X4v
"""

import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("ibm-granite/granite-3.3-2b-instruct")
model = AutoModelForCausalLM.from_pretrained("ibm-granite/granite-3.3-2b-instruct")

# Function to interact with the model
def chatbot(user_input):
    # Prepare the messages for the chatbot
    messages = [
        {"role": "user", "content": user_input},
    ]

    # Apply chat template and tokenize
    inputs = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt",
    ).to(model.device)

    # Generate response
    outputs = model.generate(**inputs, max_new_tokens=40)
    response = tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:])

    return response

# Create the Gradio interface
iface = gr.Interface(fn=chatbot,
                     inputs="text",
                     outputs="text",
                     title="IBM Granite Chatbot",
                     description="A simple chatbot powered by the IBM Granite model.")

# Launch the interface
iface.launch()

# ===============================
# Install dependencies (first run)
# ===============================
!pip install transformers accelerate gradio gTTS --quiet

# ===============================
# Imports
# ===============================
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import gradio as gr
from gtts import gTTS
import tempfile
import os

# ===============================
# Load IBM Granite model
# ===============================
model_name = "ibm-granite/granite-3.3-2b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to("cuda" if torch.cuda.is_available() else "cpu")

# ===============================
# Chat function with TTS
# ===============================
def chat_with_model(history, user_message, voice_enabled=True):
    # Append user message
    history.append({"role": "user", "content": user_message})

    # Tokenize with chat template
    inputs = tokenizer.apply_chat_template(
        history,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt",
    ).to(model.device)

    # Generate response
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=200)

    # Decode response
    reply = tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:], skip_special_tokens=True)

    # Append model reply
    history.append({"role": "assistant", "content": reply})

    # Convert to display format (tuples)
    display_history = []
    for i in range(0, len(history), 2):
        user_msg = history[i]["content"]
        assistant_msg = history[i+1]["content"] if i+1 < len(history) else ""
        display_history.append((user_msg, assistant_msg))

    # If voice enabled â†’ generate TTS
    audio_file = None
    if voice_enabled and reply.strip():
        tts = gTTS(reply)
        tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
        tts.save(tmp_file.name)
        audio_file = tmp_file.name

    return display_history, history, "", audio_file

# ===============================
# Gradio Interface
# ===============================
with gr.Blocks() as demo:
    gr.Markdown("## ðŸ¤– IBM Granite Chatbot with Audiobook Mode")
    gr.Markdown("Chat with IBM Granite model, read or listen to responses as audio.")

    chatbot = gr.Chatbot(height=400)
    state = gr.State([])  # store conversation

    with gr.Row():
        user_input = gr.Textbox(show_label=False, placeholder="Type your message here...", scale=8)
        send_btn = gr.Button("Send", scale=1)

    with gr.Row():
        voice_toggle = gr.Checkbox(value=True, label="Enable Voice (Audiobook Mode)")

    audio_output = gr.Audio(label="Assistant's Voice", type="filepath")

    # Bind send button + enter submit
    send_btn.click(fn=chat_with_model, inputs=[state, user_input, voice_toggle],
                   outputs=[chatbot, state, user_input, audio_output])
    user_input.submit(fn=chat_with_model, inputs=[state, user_input, voice_toggle],
                      outputs=[chatbot, state, user_input, audio_output])

demo.launch()

# Single-cell Colab: advanced IBM Granite chatbot + audiobook audio toolbox
# (Run in Google Colab. It installs required packages on first run.)

# -------------------------
# Install dependencies
# -------------------------
!pip install -q transformers accelerate gradio gTTS pydub librosa noisereduce soundfile PyPDF2

# -------------------------
# Imports
# -------------------------
import os, tempfile, io
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import gradio as gr
from gtts import gTTS
from pydub import AudioSegment
import soundfile as sf
import librosa
import noisereduce as nr
from PyPDF2 import PdfReader

# -------------------------
# Helper audio utilities
# -------------------------
def save_tts_to_mp3(text, lang="en"):
    """Create TTS mp3 from text and return filepath."""
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
    tts = gTTS(text, lang=lang)
    tts.save(tmp.name)
    return tmp.name

def audiosegment_to_numpy(audio: AudioSegment):
    """Return (y, sr) from pydub AudioSegment"""
    sr = audio.frame_rate
    samples = np.array(audio.get_array_of_samples())
    if audio.channels == 2:
        samples = samples.reshape((-1, 2)).mean(axis=1)  # to mono
    y = samples.astype(np.float32) / (2**(8*audio.sample_width - 1))
    return y, sr

def numpy_to_audiosegment(y, sr, sample_width=2):
    """Return pydub AudioSegment from numpy float32 [-1,1]"""
    # clip
    y_clipped = np.clip(y, -1.0, 1.0)
    int_data = (y_clipped * (2**(8*sample_width-1)-1)).astype(np.int16)
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
    sf.write(tmp.name, int_data, sr, subtype='PCM_16')
    seg = AudioSegment.from_wav(tmp.name)
    os.unlink(tmp.name)
    return seg

def change_speed_pydub(audio: AudioSegment, speed: float):
    """Change speed by altering frame_rate (simple)."""
    if speed <= 0: speed = 1.0
    new_frame_rate = int(audio.frame_rate * speed)
    sped = audio._spawn(audio.raw_data, overrides={"frame_rate": new_frame_rate})
    return sped.set_frame_rate(44100)

def pitch_shift_librosa(y, sr, n_steps):
    """Shift pitch in semitones using librosa."""
    if abs(n_steps) < 0.001:
        return y
    y_shifted = librosa.effects.pitch_shift(y, sr, n_steps=n_steps)
    return y_shifted

def add_white_noise(y, noise_level):
    """Add white noise; noise_level is fraction of signal RMS (0-1)."""
    if noise_level <= 0: return y
    rms = np.sqrt(np.mean(y**2)) + 1e-9
    noise = np.random.normal(0, 1, size=y.shape)
    noise = noise / (np.sqrt(np.mean(noise**2)) + 1e-9)
    y_noisy = y + noise * rms * noise_level
    return y_noisy

def add_background_hum(y, sr, freq=60, amplitude=0.01):
    """Add a low-frequency sine hum to the signal."""
    if amplitude <= 0 or freq <= 0:
        return y
    t = np.arange(len(y)) / sr
    hum = amplitude * np.sin(2 * np.pi * freq * t)
    return y + hum

def clip_audio(y, sr, start_sec, end_sec):
    """Clip y between start_sec and end_sec (in seconds)."""
    start_idx = int(max(0, start_sec * sr))
    end_idx = int(min(len(y), end_sec * sr)) if end_sec is not None else len(y)
    return y[start_idx:end_idx]

# -------------------------
# Load IBM Granite model & tokenizer
# -------------------------
device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "ibm-granite/granite-3.3-2b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16 if device=="cuda" else torch.float32).to(device)

# -------------------------
# Core chat + audio pipeline
# -------------------------
def generate_reply(history, user_message, tts_enable=True, tts_lang="en",
                   speed=1.0, pitch_shift=0.0, noise_add=0.0, denoise=False,
                   clip_start=0.0, clip_end=None, hum_freq=0, hum_amp=0.0):
    """
    history: list of dicts {"role","content"}
    returns display_history, history, "", audio_path_or_None
    """

    # Append user message
    history = history or []
    history.append({"role":"user", "content": user_message})

    # Prepare model inputs
    inputs = tokenizer.apply_chat_template(
        history,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt"
    ).to(device)

    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=300)

    # Decode only generated tokens
    generated = tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:], skip_special_tokens=True)
    reply = generated.strip()

    # Append assistant message
    history.append({"role":"assistant", "content": reply})

    # Prepare display for Gradio Chatbot (list of (user, assistant))
    display_history = []
    for i in range(0, len(history), 2):
        u = history[i]["content"]
        a = history[i+1]["content"] if i+1 < len(history) else ""
        display_history.append((u, a))

    audio_path = None
    # If TTS enabled, create processed audio file and return path
    if tts_enable and reply:
        # 1) Generate TTS MP3
        mp3_path = save_tts_to_mp3(reply, lang=tts_lang)

        # 2) Load into pydub
        seg = AudioSegment.from_file(mp3_path)
        os.unlink(mp3_path)

        # 3) Change speed (pydub)
        seg = change_speed_pydub(seg, speed)

        # Convert to numpy for further transforms
        y, sr = audiosegment_to_numpy(seg)

        # 4) Pitch shift (librosa)
        if abs(pitch_shift) > 0.001:
            try:
                y = pitch_shift_librosa(y, sr, n_steps=pitch_shift)
            except Exception as e:
                print("Pitch shift failed:", e)

        # 5) Add noise
        if noise_add > 0.0:
            y = add_white_noise(y, noise_add)

        # 6) Add background hum
        if hum_amp > 0 and hum_freq > 0:
            y = add_background_hum(y, sr, hum_freq, hum_amp)

        # 7) Clip start/end
        if clip_end is None:
            clip_end_val = len(y) / sr
        else:
            clip_end_val = clip_end
        y = clip_audio(y, sr, clip_start, clip_end_val)

        # 8) Denoise (optional)
        if denoise:
            try:
                y = nr.reduce_noise(y=y, sr=sr)
            except Exception as e:
                print("Denoise failed:", e)

        # 9) Convert back to AudioSegment and save final mp3
        final_seg = numpy_to_audiosegment(y, sr)
        out_tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
        final_seg.export(out_tmp.name, format="mp3")
        audio_path = out_tmp.name

    # Return for Gradio: chat display, internal history, clear textbox, audio file path
    return display_history, history, "", audio_path

# -------------------------
# Convert uploaded text or PDF to audiobook (batch)
# -------------------------
def document_to_audiobook(file_obj, voice_lang="en", chunk_chars=4000, speed=1.0, pitch_shift=0.0,
                          noise_add=0.0, denoise=False, hum_freq=0, hum_amp=0.0):
    """Converts uploaded .txt or .pdf (file_obj is a tempfile-like) into a single mp3 audiobook."""
    if file_obj is None:
        return None

    fname = file_obj.name
    text = ""
    if fname.lower().endswith(".pdf"):
        reader = PdfReader(fname)
        for p in reader.pages:
            text += p.extract_text() + "\n"
    else:
        # assume text
        with open(fname, "r", encoding="utf-8", errors="ignore") as f:
            text = f.read()

    if not text.strip():
        return None

    # Break into chunks to avoid TTS length issues
    chunks = []
    pos = 0
    while pos < len(text):
        chunk = text[pos:pos+chunk_chars]
        # try to cut on sentence end
        last_dot = chunk.rfind(".")
        if last_dot != -1 and pos + last_dot + 1 < len(text):
            chunk = text[pos:pos+last_dot+1]
            pos = pos + last_dot + 1
        else:
            pos += chunk_chars
        chunks.append(chunk.strip())

    audio_segments = []
    for chunk in chunks:
        mp3 = save_tts_to_mp3(chunk, lang=voice_lang)
        seg = AudioSegment.from_file(mp3)
        os.unlink(mp3)
        seg = change_speed_pydub(seg, speed)
        y, sr = audiosegment_to_numpy(seg)
        if abs(pitch_shift) > 1e-6:
            try:
                y = pitch_shift_librosa(y, sr, n_steps=pitch_shift)
            except Exception as e:
                print("Pitch shift chunk failed:", e)
        if noise_add>0:
            y = add_white_noise(y, noise_add)
        if hum_amp>0 and hum_freq>0:
            y = add_background_hum(y, sr, hum_freq, hum_amp)
        if denoise:
            try:
                y = nr.reduce_noise(y=y, sr=sr)
            except Exception as e:
                print("Denoise chunk failed:", e)
        seg2 = numpy_to_audiosegment(y, sr)
        audio_segments.append(seg2)

    # Concatenate and export
    final = sum(audio_segments[1:], audio_segments[0]) if len(audio_segments)>0 else None
    if final is None:
        return None
    out_tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
    final.export(out_tmp.name, format="mp3")
    return out_tmp.name

# -------------------------
# Gradio interface layout
# -------------------------
with gr.Blocks(title="IBM Granite Chatbot â€” Audiobook & Audio Toolkit") as demo:
    gr.Markdown("# ðŸŽ§ IBM Granite Chatbot â€” Audiobook Mode (Hackathon Edition)")
    gr.Markdown("Chat, listen, tweak audio (speed, pitch, noise), clip, denoise, or convert uploaded text/PDF into an audiobook.")

    with gr.Row():
        chatbot = gr.Chatbot(elem_id="chatbot", label="Conversation", height=400)
        with gr.Column(scale=1):
            state = gr.State([])

            user_input = gr.Textbox(placeholder="Type your message here...", lines=2)
            send_btn = gr.Button("Send")

            gr.Markdown("### Audiobook / Audio Controls")
            tts_checkbox = gr.Checkbox(label="Enable TTS (audio reply)", value=True)
            tts_lang = gr.Textbox(label="TTS language (gTTS code)", value="en")
            speed_slider = gr.Slider(minimum=0.5, maximum=2.0, value=1.0, label="Speed (0.5 - 2.0)")
            pitch_slider = gr.Slider(minimum=-5, maximum=5, value=0.0, step=0.5, label="Pitch shift (semitones)")
            noise_slider = gr.Slider(minimum=0.0, maximum=0.5, value=0.0, step=0.01, label="Add white noise (RMS fraction)")
            denoise_toggle = gr.Checkbox(label="Apply noise reduction", value=False)
            clip_start = gr.Slider(minimum=0.0, maximum=30.0, value=0.0, step=0.1, label="Clip start (sec)")
            clip_end = gr.Number(value=None, label="Clip end (sec) â€” leave empty for full reply")
            hum_freq = gr.Number(value=0, label="Background hum freq (Hz). 0â†’off")
            hum_amp = gr.Slider(minimum=0.0, maximum=0.1, value=0.0, step=0.001, label="Hum amplitude")

            audio_out = gr.Audio(label="Processed Assistant Audio", type="filepath")
            download_btn = gr.File(label="Download processed audio")

            # Document upload -> audiobook
            gr.Markdown("### Upload text / PDF to convert to audiobook")
            file_input = gr.File(label="Upload .txt or .pdf", file_types=[".txt", ".pdf"])
            doc_lang = gr.Textbox(label="TTS language for document", value="en")
            doc_speed = gr.Slider(0.5, 2.0, value=1.0, label="Doc playback speed")
            doc_pitch = gr.Slider(-5, 5, value=0.0, step=0.5, label="Doc pitch shift (semitones)")
            doc_noise = gr.Slider(0.0, 0.5, value=0.0, step=0.01, label="Doc add noise")
            doc_denoise = gr.Checkbox(label="Denoise doc audio", value=False)
            doc_hum_freq = gr.Number(value=0, label="Doc hum freq (Hz)")
            doc_hum_amp = gr.Slider(0.0,0.1,value=0.0,step=0.001,label="Doc hum amp")
            make_audiobook_btn = gr.Button("Make Audiobook from Document")
            doc_audio_out = gr.Audio(label="Document Audiobook", type="filepath")

    # Wire up chat send
    send_btn.click(
        fn=generate_reply,
        inputs=[state, user_input, tts_checkbox, tts_lang, speed_slider, pitch_slider, noise_slider, denoise_toggle, clip_start, clip_end, hum_freq, hum_amp],
        outputs=[chatbot, state, user_input, audio_out]
    )
    user_input.submit(
        fn=generate_reply,
        inputs=[state, user_input, tts_checkbox, tts_lang, speed_slider, pitch_slider, noise_slider, denoise_toggle, clip_start, clip_end, hum_freq, hum_amp],
        outputs=[chatbot, state, user_input, audio_out]
    )

    # Wire document -> audiobook
    make_audiobook_btn.click(
        fn=document_to_audiobook,
        inputs=[file_input, doc_lang, gr.State(), gr.State(), doc_speed, doc_pitch, doc_noise, doc_denoise, doc_hum_freq, doc_hum_amp],
        outputs=[doc_audio_out]
    )

# Launch demo
demo.launch(share=True)